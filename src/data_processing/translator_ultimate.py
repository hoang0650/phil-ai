import torch
import json
import os
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# Cáº¤U HÃŒNH
TRANSLATOR_MODEL = "Qwen/Qwen2.5-7B-Instruct" 
OUTPUT_FILE = "data/processed/combined_vietnamese_data.jsonl"
SAMPLES_TO_TRANSLATE = 5000 

def load_translator():
    print(f">>> ğŸ”„ Äang táº£i Translator: {TRANSLATOR_MODEL}...")
    tokenizer = AutoTokenizer.from_pretrained(TRANSLATOR_MODEL)
    model = AutoModelForCausalLM.from_pretrained(
        TRANSLATOR_MODEL, 
        torch_dtype=torch.float16, 
        device_map="auto"
    )
    return model, tokenizer

def translate_text(model, tokenizer, text):
    # Prompt dÃ nh riÃªng cho Qwen
    messages = [
        {"role": "system", "content": "Báº¡n lÃ  má»™t biÃªn dá»‹ch viÃªn ká»¹ thuáº­t chuyÃªn nghiá»‡p. HÃ£y dá»‹ch Ä‘oáº¡n vÄƒn báº£n sau sang Tiáº¿ng Viá»‡t. QUY Táº®C: Giá»¯ nguyÃªn táº¥t cáº£ Code, tÃªn biáº¿n, tÃªn hÃ m vÃ  thuáº­t ngá»¯ tiáº¿ng Anh. Chá»‰ dá»‹ch pháº§n lá»i giáº£i thÃ­ch."},
        {"role": "user", "content": text[:2000]}
    ]
    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer([text_input], return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
        
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "assistant" in response:
        return response.split("assistant")[-1].strip()
    return response

def run():
    os.makedirs("data/processed", exist_ok=True)
    model, tokenizer = load_translator()
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        # 1. Glaive Dataset
        ds = load_dataset("glaiveai/glaive-code-assistant-v2", split=f"train[:{SAMPLES_TO_TRANSLATE}]")
        for item in tqdm(ds, desc="Translating Glaive"):
            try:
                vn_instr = translate_text(model, tokenizer, item['question'])
                # LÆ°u cáº£ báº£n gá»‘c vÃ  báº£n dá»‹ch Ä‘á»ƒ train song ngá»¯
                record = {"instruction": vn_instr, "output": item['answer'], "source": "glaive_vn"}
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
            except: continue

        # 2. Evol Dataset
        ds = load_dataset("nickrosh/Evol-Instruct-Code-80k-v1", split=f"train[:{SAMPLES_TO_TRANSLATE}]")
        for item in tqdm(ds, desc="Translating Evol"):
            try:
                vn_instr = translate_text(model, tokenizer, item['instruction'])
                record = {"instruction": vn_instr, "output": item['output'], "source": "evol_vn"}
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
            except: continue
            
    print(f">>> âœ… ÄÃ£ dá»‹ch xong! File lÆ°u táº¡i: {OUTPUT_FILE}")

if __name__ == "__main__":
    run()