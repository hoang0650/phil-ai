# üè≠ Phil AI Training & Inference Factory

> **"X∆∞·ªüng ƒë√∫c" Tr√≠ tu·ªá nh√¢n t·∫°o cho Phil - Th·ª±c th·ªÉ s·ªë Vi·ªát Nam (Vietnam's Sovereign Digital Human).**
> D·ª± √°n n√†y chuy√™n bi·ªát h√≥a ƒë·ªÉ Fine-tune c√°c m√¥ h√¨nh SOTA (State-of-the-Art) h·∫°ng n·∫∑ng tr√™n ph·∫ßn c·ª©ng **NVIDIA H200 SXM (141GB VRAM)** v√† cung c·∫•p gi·∫£i ph√°p Inference ƒëa n·ªÅn t·∫£ng.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![Hardware](https://img.shields.io/badge/Hardware-H200_SXM-green.svg)
![Framework](https://img.shields.io/badge/Framework-Unsloth%20%7C%20LLaMA--Factory-red)
![Status](https://img.shields.io/badge/Status-Operational-brightgreen)

---

## üß† Ki·∫øn Tr√∫c "T·ª© Tr·ª•" (The Big Four)

H·ªá th·ªëng n√†y kh√¥ng t·∫°o ra m·ªôt chatbot, m√† t·∫°o ra 4 th√†nh ph·∫ßn c·∫•u th√†nh m·ªôt con ng∆∞·ªùi k·ªπ thu·∫≠t s·ªë:

| Th√†nh ph·∫ßn | Vai tr√≤ | Model G·ªëc (Base) | K·ªπ thu·∫≠t Train | Dataset Ch√≠nh |
| :--- | :--- | :--- | :--- | :--- |
| **1. Brain** | T∆∞ duy, Code, Logic | `DeepSeek-R1-Distill-Llama-70B` | QLoRA 4-bit (Unsloth) | Glaive + Evol + **Vietnamese Translated** |
| **2. Eyes** | Nh√¨n, OCR, UI/UX | `OpenGVLab/InternVL2-76B` | QLoRA 4-bit (LLaMA-Factory) | OCR-VQA + Tech Screenshots |
| **3. Ears** | Nghe thu·∫≠t ng·ªØ IT | `OpenAI/Whisper-Large-v3` | LoRA Adapter | Youtube Tech Talks (Vietnamese) |
| **4. Mouth** | Gi·ªçng n√≥i ƒë·ªãnh danh | `F5-TTS (E2-TTS)` | Flow Matching | **Phil Studio Voice** (Custom) |

---

## üöÄ T√≠nh nƒÉng m·ªõi: ƒêa n·ªÅn t·∫£ng Inference

H·ªá th·ªëng hi·ªán ƒë√£ t√≠ch h·ª£p c√°c engine inference m·∫°nh m·∫Ω nh·∫•t ƒë·ªÉ t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô v√† t√†i nguy√™n cho vi·ªác tri·ªÉn khai AI Sale Agent:

1.  **vLLM**: T·ªëi ∆∞u h√≥a throughput cho GPU NVIDIA, h·ªó tr·ª£ PagedAttention.
2.  **Text Generation Inference (TGI)**: Gi·∫£i ph√°p t·ª´ HuggingFace cho vi·ªác tri·ªÉn khai production.
3.  **llama.cpp**: Ch·∫°y m√¥ h√¨nh tr√™n CPU ho·∫∑c GPU v·ªõi ƒë·ªãnh d·∫°ng GGUF, c·ª±c k·ª≥ ti·∫øt ki·ªám t√†i nguy√™n.
4.  **Transformers**: Backend m·∫∑c ƒë·ªãnh cho vi·ªác th·ª≠ nghi·ªám nhanh.

---

## üîÑ Chi·∫øn l∆∞·ª£c "Cu·ªën chi·∫øu" (Rolling Strategy)

ƒê·ªÉ ti·∫øt ki·ªám chi ph√≠ thu√™ ·ªï c·ª©ng tr√™n RunPod, d·ª± √°n tri·ªÉn khai chi·∫øn l∆∞·ª£c **"Cu·ªën chi·∫øu"** trong script `run_all.sh`:
- **Quy tr√¨nh**: T·∫£i Model G·ªëc -> Hu·∫•n luy·ªán (Fine-tune) -> Upload k·∫øt qu·∫£ l√™n HuggingFace -> **X√≥a Model G·ªëc & Cache** -> Chuy·ªÉn sang model ti·∫øp theo.
- **L·ª£i √≠ch**: Gi·∫£m y√™u c·∫ßu dung l∆∞·ª£ng Disk t·ª´ >500GB xu·ªëng c√≤n kho·∫£ng 200GB, ngay c·∫£ khi l√†m vi·ªác v·ªõi c√°c model kh·ªïng l·ªì nh∆∞ DeepSeek 70B hay InternVL2 76B.

---

## üõ†Ô∏è Y√™u C·∫ßu H·ªá Th·ªëng

D·ª± √°n n√†y ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho **Runpod H200 Pod**.

* **GPU:** 1x NVIDIA H200 SXM (141GB VRAM).
* **Disk:** T·ªëi thi·ªÉu 200GB Container Disk / Volume (Nh·ªù chi·∫øn l∆∞·ª£c Cu·ªën chi·∫øu).
* **RAM:** 128GB+.
* **Internet:** Runpod Datacenter Speed (Download Dataset ~10Gbps).

---

## üìÇ C·∫•u Tr√∫c D·ª± √Ån

```text
ai-sale-agent/
‚îú‚îÄ‚îÄ configs/                   # C·∫•u h√¨nh Hyperparameters (YAML)
‚îú‚îÄ‚îÄ data/                      # Kho d·ªØ li·ªáu
‚îú‚îÄ‚îÄ scripts/                   # Shell scripts ƒëi·ªÅu khi·ªÉn & Export GGUF
‚îú‚îÄ‚îÄ src/                       # M√£ ngu·ªìn Python
‚îÇ   ‚îú‚îÄ‚îÄ data_processing/       # Module d·ªãch thu·∫≠t & x·ª≠ l√Ω Audio
‚îÇ   ‚îú‚îÄ‚îÄ training/              # Module train Core (Unsloth & F5-TTS)
‚îÇ   ‚îî‚îÄ‚îÄ inference/             # Engine x·ª≠ l√Ω suy lu·∫≠n ƒëa n·ªÅn t·∫£ng (M·ªöI)
‚îú‚îÄ‚îÄ server.py                  # API Server t√≠ch h·ª£p RAG & AI Agent
‚îî‚îÄ‚îÄ requirements.txt           # Dependencies
```

---

## üöÄ H∆∞·ªõng D·∫´n V·∫≠n H√†nh (Step-by-Step)

### B∆∞·ªõc 1: Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng
K·∫øt n·ªëi SSH v√†o Runpod v√† ch·∫°y:
```bash
pip install -r requirements.txt
### Khai b√°o nhi·ªÅu bi·∫øn m√¥i tr∆∞·ªùng
### C√°ch 1
echo "HF_TOKEN=hf_write_token_here" > .env
echo "WANDB_API_KEY=write_wandb_api_key" >> .env
### C√°ch 2
cat << EOF > .env
HF_TOKEN=hf_write_token_here
WANDB_API_KEY=write_wandb_api_key
EOF
```

### B∆∞·ªõc 2: Ch·∫°y to√†n b·ªô quy tr√¨nh (Chi·∫øn l∆∞·ª£c Cu·ªën chi·∫øu)
```bash
chmod +x scripts/*.sh
./scripts/run_all.sh
```

### B∆∞·ªõc 3: Tri·ªÉn khai Inference Server
B·∫°n c√≥ th·ªÉ ch·ªçn engine th√¥ng qua bi·∫øn m√¥i tr∆∞·ªùng:

**Ch·∫°y v·ªõi vLLM:**
```bash
export ENGINE_TYPE=vllm
export MODEL_PATH=./path-to-your-model
python server.py
```

**Ch·∫°y v·ªõi llama.cpp (GGUF):**
```bash
export ENGINE_TYPE=llama.cpp
export MODEL_PATH=./model.gguf
python server.py
```

---

## ‚òÅÔ∏è Tri·ªÉn khai l√™n RunPod

Xem chi ti·∫øt trong file [RunPod_Deployment_Guide.docx](./RunPod_Deployment_Guide.docx) ƒë·ªÉ bi·∫øt c√°ch thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng GPU tr√™n RunPod.

---

## üì¶ Output Artifacts (S·∫£n ph·∫©m ƒë·∫ßu ra)
Sau khi train xong, c√°c model s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông upload l√™n HuggingFace c·ªßa b·∫°n.

---

## üìÑ Gi·∫•y ph√©p
MIT License.
