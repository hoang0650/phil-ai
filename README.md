# üè≠ Phil AI Training & Inference Factory

> **"X∆∞·ªüng ƒë√∫c" Tr√≠ tu·ªá nh√¢n t·∫°o cho Phil - Th·ª±c th·ªÉ s·ªë Vi·ªát Nam (Vietnam's Sovereign Digital Human).**
> D·ª± √°n n√†y chuy√™n bi·ªát h√≥a ƒë·ªÉ Fine-tune c√°c m√¥ h√¨nh SOTA (State-of-the-Art) h·∫°ng n·∫∑ng tr√™n ph·∫ßn c·ª©ng **NVIDIA H200 SXM (141GB VRAM)** v√† cung c·∫•p gi·∫£i ph√°p Inference ƒëa n·ªÅn t·∫£ng.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![Hardware](https://img.shields.io/badge/Hardware-H200_SXM-green.svg)
![Framework](https://img.shields.io/badge/Framework-Unsloth%20%7C%20LLaMA--Factory-red)
![Status](https://img.shields.io/badge/Status-Operational-brightgreen)

---

## üß† Ki·∫øn Tr√∫c "T·ª© Tr·ª•" (The Big Four)

H·ªá th·ªëng n√†y kh√¥ng t·∫°o ra m·ªôt chatbot, m√† t·∫°o ra 4 th√†nh ph·∫ßn c·∫•u th√†nh m·ªôt con ng∆∞·ªùi k·ªπ thu·∫≠t s·ªë:

| Th√†nh ph·∫ßn | Vai tr√≤ | Model G·ªëc (Base) | K·ªπ thu·∫≠t Train | Dataset Ch√≠nh |
| :--- | :--- | :--- | :--- | :--- |
| **1. Brain** | T∆∞ duy, Code, Logic | `DeepSeek-R1-Distill-Llama-70B` | QLoRA 4-bit (Unsloth) | Glaive + Evol + **Vietnamese Translated** |
| **2. Eyes** | Nh√¨n, OCR, UI/UX | `OpenGVLab/InternVL2-76B` | QLoRA 4-bit (LLaMA-Factory) | OCR-VQA + Tech Screenshots |
| **3. Ears** | Nghe thu·∫≠t ng·ªØ IT | `OpenAI/Whisper-Large-v3` | LoRA Adapter | Youtube Tech Talks (Vietnamese) |
| **4. Mouth** | Gi·ªçng n√≥i ƒë·ªãnh danh | `F5-TTS (E2-TTS)` | Flow Matching | **Phil Studio Voice** (Custom) |

---

## üöÄ T√≠nh nƒÉng m·ªõi: ƒêa n·ªÅn t·∫£ng Inference

H·ªá th·ªëng hi·ªán ƒë√£ t√≠ch h·ª£p c√°c engine inference m·∫°nh m·∫Ω nh·∫•t ƒë·ªÉ t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô v√† t√†i nguy√™n cho vi·ªác tri·ªÉn khai AI Sale Agent:

1.  **vLLM**: T·ªëi ∆∞u h√≥a throughput cho GPU NVIDIA, h·ªó tr·ª£ PagedAttention.
2.  **Text Generation Inference (TGI)**: Gi·∫£i ph√°p t·ª´ HuggingFace cho vi·ªác tri·ªÉn khai production.
3.  **llama.cpp**: Ch·∫°y m√¥ h√¨nh tr√™n CPU ho·∫∑c GPU v·ªõi ƒë·ªãnh d·∫°ng GGUF, c·ª±c k·ª≥ ti·∫øt ki·ªám t√†i nguy√™n.
4.  **Transformers**: Backend m·∫∑c ƒë·ªãnh cho vi·ªác th·ª≠ nghi·ªám nhanh.

---

## üîÑ Chi·∫øn l∆∞·ª£c "Cu·ªën chi·∫øu" (Rolling Strategy)

ƒê·ªÉ ti·∫øt ki·ªám chi ph√≠ thu√™ ·ªï c·ª©ng tr√™n RunPod, d·ª± √°n tri·ªÉn khai chi·∫øn l∆∞·ª£c **"Cu·ªën chi·∫øu"** trong script `run_all.sh`:
- **Quy tr√¨nh**: T·∫£i Model G·ªëc -> Hu·∫•n luy·ªán (Fine-tune) -> Upload k·∫øt qu·∫£ l√™n HuggingFace -> **X√≥a Model G·ªëc & Cache** -> Chuy·ªÉn sang model ti·∫øp theo.
- **L·ª£i √≠ch**: Gi·∫£m y√™u c·∫ßu dung l∆∞·ª£ng Disk t·ª´ >500GB xu·ªëng c√≤n kho·∫£ng 200GB, ngay c·∫£ khi l√†m vi·ªác v·ªõi c√°c model kh·ªïng l·ªì nh∆∞ DeepSeek 70B hay InternVL2 76B.

---

## üîó T√≠ch h·ª£p v·ªõi Phil-CLI

Phil-AI hi·ªán l√† n·ªÅn t·∫£ng tr√≠ tu·ªá nh√¢n t·∫°o c·ªët l√µi cho **Phil-CLI**, cung c·∫•p c√°c model t·ª± train thay th·∫ø ho√†n to√†n c√°c API b√™n ngo√†i nh∆∞ Anthropic.

### C√°ch t√≠ch h·ª£p:
1. **Brain Model**: Cung c·∫•p kh·∫£ nƒÉng suy lu·∫≠n v√† l·∫≠p tr√¨nh (thay th·∫ø GPT-4/Claude)
2. **Vision Model**: X·ª≠ l√Ω h√¨nh ·∫£nh v√† OCR (thay th·∫ø GPT-4V)
3. **Audio Models**: Chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i sang vƒÉn b·∫£n v√† ng∆∞·ª£c l·∫°i
4. **Security Integration**: T√≠ch h·ª£p v·ªõi sandbox v√† security policies c·ªßa Phil-CLI

### L·ª£i √≠ch:
- ‚úÖ **ƒê·ªôc l·∫≠p ho√†n to√†n**: Kh√¥ng ph·ª• thu·ªôc v√†o API b√™n ngo√†i
- ‚úÖ **B·∫£o m·∫≠t cao**: D·ªØ li·ªáu kh√¥ng r·ªùi kh·ªèi h·ªá th·ªëng
- ‚úÖ **T·ªëi ∆∞u chi ph√≠**: Kh√¥ng c√≥ chi ph√≠ API ƒë·ªãnh k·ª≥
- ‚úÖ **T√πy ch·ªânh linh ho·∫°t**: C√≥ th·ªÉ fine-tune theo nhu c·∫ßu ri√™ng

---

## üõ†Ô∏è Y√™u C·∫ßu H·ªá Th·ªëng

D·ª± √°n n√†y ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho **Runpod H200 Pod**.

* **GPU:** 1x NVIDIA H200 SXM (141GB VRAM).
* **Disk:** T·ªëi thi·ªÉu 200GB Container Disk / Volume (Nh·ªù chi·∫øn l∆∞·ª£c Cu·ªën chi·∫øu).
* **RAM:** 128GB+.
* **Internet:** Runpod Datacenter Speed (Download Dataset ~10Gbps).

---

## üìÇ C·∫•u Tr√∫c D·ª± √Ån

```text
ai-sale-agent/
‚îú‚îÄ‚îÄ üè≠ phil_training_factory/    # X∆∞·ªüng luy·ªán Model (Ch·∫°y 1 l·∫ßn)
‚îÇ   ‚îú‚îÄ‚îÄ configs/                 # Ch·ªânh tham s·ªë train
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                 # Script t·ª± ƒë·ªông h√≥a
‚îÇ   ‚îî‚îÄ‚îÄ outputs/                 # N∆°i Model ra l√≤
‚îÇ
‚îú‚îÄ‚îÄ üöÄ phil_inference/           # Server tri·ªÉn khai (Ch·∫°y 24/7)
‚îÇ   ‚îú‚îÄ‚îÄ config/                  # Ch·ªçn backend (vLLM/TGI)
‚îÇ   ‚îú‚îÄ‚îÄ src/                     # API Gateway Logic
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml       # H·∫° t·∫ßng container
```

---

## üöÄ H∆∞·ªõng D·∫´n V·∫≠n H√†nh (Step-by-Step)

### B∆∞·ªõc 1: Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng
Y√™u c·∫ßu: NVIDIA H200 (141GB VRAM).
K·∫øt n·ªëi SSH v√†o Runpod v√† ch·∫°y:
```bash
git clone https://github.com/hoang0650/phil-ai
cd phil_training_factory
pip install -r requirements.txt
### Khai b√°o nhi·ªÅu bi·∫øn m√¥i tr∆∞·ªùng
### C√°ch 1
cp .env.example .env
### C√°ch 2
echo "HF_TOKEN=hf_write_token_here" > .env
echo "WANDB_API_KEY=write_wandb_api_key" >> .env
### C√°ch 3
cat << EOF > .env
HF_TOKEN=hf_write_token_here
WANDB_API_KEY=write_wandb_api_key
EOF
```

### B∆∞·ªõc 2: Ch·∫°y to√†n b·ªô quy tr√¨nh (Chi·∫øn l∆∞·ª£c Cu·ªën chi·∫øu)
B·∫°n ch·ªâ c·∫ßn ch·∫°y 1 l·ªánh duy nh·∫•t ƒë·ªÉ train to√†n b·ªô 4 model:
```bash
# Script n√†y s·∫Ω t·ª± ƒë·ªông:
# 1. T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu (D·ªãch sang ti·∫øng Vi·ªát)
# 2. Train Brain (DeepSeek 70B)
# 3. Train Vision (InternVL2 76B)
# 4. Train Audio (Whisper + F5-TTS)
chmod +x scripts/*.sh
./scripts/run_all.sh
```
Sau khi ch·∫°y xong, k·∫øt qu·∫£ s·∫Ω n·∫±m trong th∆∞ m·ª•c `phil_training_factory/outputs/`.

### B∆∞·ªõc 3: CHUY·ªÇN ƒê·ªîI & TRI·ªÇN KHAI (PHIL INFERENCE)
B·∫°n c√≥ th·ªÉ ch·ªçn engine th√¥ng qua bi·∫øn m√¥i tr∆∞·ªùng:

**Chuy·ªÉn Model sang Inference**
Ch√∫ng ta c·∫ßn copy model t·ª´ "X∆∞·ªüng" sang th∆∞ m·ª•c "Tri·ªÉn khai".
```bash
mkdir -p phil_inference/models
cp -r phil_training_factory/outputs/* phil_inference/models/
```

**L·ª±a ch·ªçn Backend (vLLM vs TGI vs llama.cpp)**
M·ªü file `phil_inference/config/model_config.yaml` ƒë·ªÉ c·∫•u h√¨nh.
**Option A: D√πng vLLM (Khuy√™n d√πng cho H200 - T·ªëc ƒë·ªô cao nh·∫•t)**
```yaml
brain:
  active_backend: "vllm"
```
∆Øu ƒëi·ªÉm: H·ªó tr·ª£ PagedAttention, throughput c·ª±c cao.
**Option B: D√πng llama.cpp (N·∫øu mu·ªën ch·∫°y ti·∫øt ki·ªám VRAM)**
Tr∆∞·ªõc ti√™n, c·∫ßn convert model sang GGUF:
```bash
# T·∫°i th∆∞ m·ª•c phil_training_factory
python3 convert_hf_to_gguf.py outputs/Phil-70B-Coder-N2 --outfile models/phil-brain.gguf
```
Sau ƒë√≥ s·ª≠a config:
```yaml
brain:
  active_backend: "llamacpp"
```
**Kh·ªüi ƒë·ªông Server**
```bash
cd phil_inference
docker-compose up -d --build
```
H·ªá th·ªëng s·∫Ω kh·ªüi ƒë·ªông c√°c container:
* vllm-brain (Port 8000)
* vllm-vision (Port 8001)
* phil-gateway (Port 3000 - API ch√≠nh)

**S·ª¨ D·ª§NG (PHIL CLI)**
Tr√™n m√°y t√≠nh c√° nh√¢n c·ªßa b·∫°n:
```bash
cd phil-cli
pip install requests

# Chat v·ªõi Phil
python phil.py chat "Phil ∆°i, vi·∫øt cho anh code Python gi·∫£i thu·∫≠t Dijkstra"

# Nh·ªù Phil nh√¨n l·ªói
python phil.py see ./error_screenshot.png --prompt "L·ªói n√†y s·ª≠a sao em?"
```

---

## ‚òÅÔ∏è Tri·ªÉn khai l√™n RunPod

Xem chi ti·∫øt trong file [RunPod_Deployment_Guide.docx](https://docs.google.com/document/d/1JeqsSHzRNZQ1dpyWgQYaZKOmHqpz86a5/edit?usp=sharing&ouid=111551674717295623221&rtpof=true&sd=true) ƒë·ªÉ bi·∫øt c√°ch thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng GPU tr√™n RunPod.

---

## üì¶ Output Artifacts (S·∫£n ph·∫©m ƒë·∫ßu ra)
Sau khi train xong, c√°c model s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông upload l√™n HuggingFace c·ªßa b·∫°n.

---

## üîß T√≠ch h·ª£p v·ªõi Phil-CLI

ƒê·ªÉ s·ª≠ d·ª•ng Phil-AI trong Phil-CLI, c·∫•u h√¨nh c√°c endpoints trong file `config.py`:

```python
# Phil-AI Model Endpoints
BRAIN_MODEL_ENDPOINT = "http://localhost:8000/v1"
VISION_MODEL_ENDPOINT = "http://localhost:8001/v1"
EARS_MODEL_ENDPOINT = "http://localhost:8002/v1"
MOUTH_MODEL_ENDPOINT = "http://localhost:8003/v1"
```

---

## üìÑ Gi·∫•y ph√©p
Code d·ª± √°n tu√¢n th·ªß MIT/Apache 2.0. D·ªØ li·ªáu training ƒë√£ ƒë∆∞·ª£c l·ªçc ƒë·ªÉ ƒë·∫£m b·∫£o quy·ªÅn th∆∞∆°ng m·∫°i (Commercial Use).